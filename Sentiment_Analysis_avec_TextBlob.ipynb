{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis avec TextBlob.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YcNyjKxovVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f419bf-eac9-441a-97df-b0e4ba2512d4"
      },
      "source": [
        "!pip install newspaper3K # Pour scrapping data\n",
        "!pip install nltk # Natural Langage ToolKit pour le cleaning des données textuelles\n",
        "!pip install textblob # Pour le sentiment analysis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3K\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 10.6MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 12.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (2.8.1)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (4.6.3)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (3.13)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (2.23.0)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (7.1.2)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3K) (4.2.6)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->newspaper3K) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3K) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3K) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3K) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3K) (2020.12.5)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3K) (3.0.12)\n",
            "Building wheels for collected packages: feedfinder2, tinysegmenter, jieba3k, sgmllib3k\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=8c788d41be44ed5d9ba096463c187d6d03ed4503fdef3cddd8f658b874d5dc20\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=4f66444cfa92e4ce2af11396a2eb9832406c8f067ac3c5b95b13beb2ddb2b05c\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=b2b4932931360d1b84f13efe71cd364eef1884c31edcfd9decc9f120a9e795ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=7cedb924045f7b3fd497c8c77c5a45acead4f1abe9fff82aa25f7d5ca32f3f4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built feedfinder2 tinysegmenter jieba3k sgmllib3k\n",
            "Installing collected packages: feedfinder2, tinysegmenter, cssselect, sgmllib3k, feedparser, jieba3k, requests-file, tldextract, newspaper3K\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3K-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3yLQS03pJRs"
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from newspaper import Article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkAJkFnrqQ_x"
      },
      "source": [
        "# **Scrappind Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6wZomFhpixr"
      },
      "source": [
        "url='https://fr.wikipedia.org/wiki/Apprentissage_automatique'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtgw_k8npiyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a94fe8-080a-4804-e9ac-b218b5574334"
      },
      "source": [
        "article=Article(url) # Catser l'URL dans un objet de type Article\n",
        "article.download() # Télécharger l'arcticle\n",
        "article.parse() #Parser le contenu du body dans la page HTML\n",
        "nltk.download('punkt') #on aura besoin dans nlp() de remove la ponctuation donc on devrait la télécharger de la bibiliothèque NLTK\n",
        "article.nlp() # Rendre le Texte sous forme Objet NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bGSPa5OqKjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24a6ac1-2094-460a-833b-38bfa4a5b142"
      },
      "source": [
        "print(article.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mldemos). Par l'entraînement, ici supervisé, sur un grand nombre de mesures, il devient facile à un programme d'apprentissage automatique de reconnaître des formes, même complexes, et d'y classifier ensuite de nouveaux points (exemple d'usage du programme).\n",
            "\n",
            "L'apprentissage automatique[1],[2] (en anglais : machine learning, litt. « apprentissage machine[1],[2] »), apprentissage artificiel[1] ou apprentissage statistique est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes.\n",
            "\n",
            "L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.\n",
            "\n",
            "Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement[3] si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.\n",
            "\n",
            "Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.\n",
            "\n",
            "La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936[4], qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950[5], dans lequel il développe, entre autres, le test de Turing.\n",
            "\n",
            "En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux[6].\n",
            "\n",
            "Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis[7],[8].\n",
            "\n",
            "Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy![9]. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel[10].\n",
            "\n",
            "Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.\n",
            "\n",
            "En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube[11],[12].\n",
            "\n",
            "En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans[13].\n",
            "\n",
            "En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde[14].\n",
            "\n",
            "En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès[15],[16].\n",
            "\n",
            "L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.\n",
            "\n",
            "L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle[17].\n",
            "\n",
            "Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…\n",
            "\n",
            "L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc. ) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire).\n",
            "\n",
            "Exemples :\n",
            "\n",
            "un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace [réf. nécessaire] ;\n",
            "\n",
            "; la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine [ 18 ] , [ 19 ] , et ceux utilisés pour la reconnaissance d'écriture ou OCR.\n",
            "\n",
            "Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient.\n",
            "\n",
            "Apprentissage supervisé Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classification ou de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle à partir des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées (on parle alors d'apprentissage supervisé probabiliste). ex. : L'points communs détectés avec les exemples), le système peut catégoriser de nouveaux patients au vu de leurs L' analyse discriminante linéaire ou les SVM en sont des exemples typiques. Autre exemple : en fonction dedétectés avec les symptômes d'autres patients connus (les), le système peut catégoriser de nouveaux patients au vu de leurs analyses médicales en risque estimé ( probabilité ) de développer telle ou telle maladie.\n",
            "\n",
            "Apprentissage non supervisé Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.\n",
            "\n",
            "Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).\n",
            "\n",
            "Cette méthode est souvent source de sérendipité. ex. : Pour un etc. ). Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique , habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques ( métaux lourds toxines telle que l' aflatoxine ).\n",
            "\n",
            "Apprentissage semi-supervisé Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.\n",
            "\n",
            "Apprentissage partiellement supervisé Probabiliste ou non, quand l'étiquetage des données est partiel [ 20 ] . C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).:\n",
            "\n",
            "Apprentissage par transfert [ 23 ] L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. La question qui se pose est : comment identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis comment transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s) ?\n",
            "\n",
            "Ce sont, dans ce domaine :\n",
            "\n",
            "Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. Le choix d'un algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs…), du volume et de la nature des données. Ces modèles reposent souvent sur des modèles statistiques.\n",
            "\n",
            "Facteurs de pertinence et d'efficacité [ modifier | modifier le code ]\n",
            "\n",
            "La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).\n",
            "\n",
            "Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données :\n",
            "\n",
            "nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;\n",
            "\n",
            "(moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ; nombre et qualité des attributs décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc. ) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ;\n",
            "\n",
            "décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, ) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ; pourcentage de données renseignées et manquantes ;\n",
            "\n",
            "et manquantes ; bruit : le nombre et la « localisation » des valeurs douteuses (erreurs potentielles, valeurs aberrantes…) ou naturellement non-conformes au pattern de distribution générale des « exemples » sur leur espace de distribution impacteront sur la qualité de l'analyse.\n",
            "\n",
            "Étapes d'un projet d'apprentissage automatique [ modifier | modifier le code ]\n",
            "\n",
            "L'apprentissage automatique ne se résume pas à un ensemble d'algorithmes, mais suit une succession d'étapes[27],[28].\n",
            "\n",
            "Définir le problème à résoudre Acquérir des données : l'algorithme se nourrissant des données en entrée, c'est une étape importante. Il en va de la réussite du projet, de récolter des données pertinentes et en quantité et qualité suffisantes, et en évitant tout biais dans leur représentativité. Analyser et explorer les données Préparer et nettoyer les données : les données recueillies doivent être retouchées avant utilisation. En effet, certains attributs sont inutiles, d’autre doivent être modifiés afin d’être compris par l’algorithme, et certains éléments sont inutilisables car leurs données sont incomplètes. Plusieurs techniques telles que la visualisation de données, la transformation de données (en) ou encore la normalisation sont alors employées. Ingénierie ou extraction de caractéristiques: les attributs peuvent être combinés entre eux pour en créer de nouveaux plus pertinents et efficaces pour l'entrainement du modèle. Choisir ou construire un modèle d’apprentissage: un large choix d'algorithmes existe, et il faut en choisir un adapté au problème et aux données. Entrainer, évaluer et optimiser : l'algorithme d'apprentissage automatique est entraîné et validé sur un premier jeu de données pour optimiser ses hyperparamètres. Test: puis il est évalué sur un deuxième ensemble de données de test afin de vérifier qu'il est efficace avec un jeu de donnée indépendant des données d'entrainement, et pour vérifier qu'il ne fasse pas de surapprentissage. Déployer : le modèle est alors déployé en production pour faire des prédictions, et potentiellement utiliser les nouvelles données en entrée pour se ré-entraîner et être amélioré.\n",
            "\n",
            "La plupart de ces étapes se retrouvent dans les méthodes et processus de projet KDD, CRISP-DM et SEMMA[29], qui concernent les projets d'exploration de données.\n",
            "\n",
            "Application à la voiture autonome [ modifier | modifier le code ]\n",
            "\n",
            "La voiture autonome paraît en 2016 réalisable grâce à l’apprentissage automatique et les énormes quantités de données générées par la flotte automobile, de plus en plus connectée. Contrairement aux algorithmes classiques (qui suivent un ensemble de règles prédéterminées), l’apprentissage automatique apprend ses propres règles[30].\n",
            "\n",
            "Les principaux innovateurs dans le domaine insistent sur le fait que le progrès provient de l’automatisation des processus. Ceci présente le défaut que le processus d’apprentissage automatique devient privatisé et obscur. Privatisé, car les algorithmes d’AA constituent des gigantesques opportunités économiques, et obscurs car leur compréhension passe derrière leur optimisation. Cette évolution peut potentiellement nuire à la confiance du public envers l’apprentissage automatique, mais surtout au potentiel à long terme de techniques très prometteuses[31].\n",
            "\n",
            "La voiture autonome présente un cadre test pour confronter l’apprentissage automatique à la société. En effet, ce n’est pas seulement l’algorithme qui se forme à la circulation routière et ses règles, mais aussi l’inverse. Le principe de responsabilité est remis en cause par l’apprentissage automatique, car l’algorithme n’est plus écrit mais apprend et développe une sorte d’intuition numérique. Les créateurs d’algorithmes ne sont plus en mesure de comprendre les « décisions » prises par leurs algorithmes, ceci par construction mathématique même de l’algorithme d’apprentissage automatique[32].\n",
            "\n",
            "Dans le cas de l’AA et les voitures autonomes, la question de la responsabilité en cas d’accident se pose. La société doit apporter une réponse à cette question, avec différentes approches possibles. Aux États-Unis, il existe la tendance à juger une technologie par la qualité du résultat qu’elle produit, alors qu’en Europe le principe de précaution est appliqué, et on y a plus tendance à juger une nouvelle technologie par rapport aux précédentes, en évaluant les différences par rapport à ce qui est déjà connu. Des processus d’évaluation de risques sont en cours en Europe et aux États-Unis[31].\n",
            "\n",
            "La question de responsabilité est d’autant plus compliquée que la priorité chez les concepteurs réside en la conception d’un algorithme optimal, et non pas de le comprendre. L’interprétabilité des algorithmes est nécessaire pour en comprendre les décisions, notamment lorsque ces décisions ont un impact profond sur la vie des individus. Cette notion d’interprétabilité, c’est-à-dire de la capacité de comprendre pourquoi et comment un algorithme agit, est aussi sujette à interprétation.\n",
            "\n",
            "La question de l’accessibilité des données est sujette à controverse : dans le cas des voitures autonomes, certains défendent l’accès public aux données, ce qui permettrait un meilleur apprentissage aux algorithmes et ne concentrerait pas cet « or numérique » dans les mains d’une poignée d’individus, de plus d’autres militent pour la privatisation des données au nom du libre marché, sans négliger le fait que des bonnes données constituent un avantage compétitif et donc économique[31],[33].\n",
            "\n",
            "La question des choix moraux liés aux décisions laissées aux algorithmes d'AA et aux voitures autonomes en cas de situations dangereuses ou mortelles se pose aussi. Par exemple en cas de défaillance des freins du véhicule, et d'accident inévitable, quelles vies sont à sauver en priorité: celle des passagers ou bien celle des piétons traversant la rue? [34]\n",
            "\n",
            "Dans les années 2000-2010, l'apprentissage automatique est encore une technologie émergente, mais polyvalente, qui est par nature théoriquement capable d'accélérer le rythme de l'automatisation et de l'autoaprentissage lui-même. Combiné à l'apparition de nouveaux moyens de produire, stocker et faire circuler l'énergie, ainsi qu'à l'informatique ubiquiste, il pourrait bouleverser les technologies et la société (comme l'ont fait la machine à vapeur et l'électricité, puis le pétrole et l'informatique lors des révolutions industrielles précédentes. L'apprentissage automatique pourrait générer des innovations et des capacités inattendues, mais avec un risque selon certains observateurs de perte de maitrise de la part des humains sur de nombreuses tâches qu'ils ne pourront plus comprendre et qui seront faites en routine par des entités informatiques et robotisées. Ceci laisse envisager des impacts spécifiques complexes et encore impossibles à évaluer sur l'emploi, le travail et plus largement l'économie et les inégalités.\n",
            "\n",
            "Selon le journal Science fin 2017 : « Les effets sur l'emploi sont plus complexes que la simple question du remplacement et des substitutions soulignées par certains. Bien que les effets économiques du BA soient relativement limités aujourd'hui et que nous ne soyons pas confrontés à une « fin du travail » imminente comme cela est parfois proclamé, les implications pour l'économie et la main-d'œuvre sont profondes »[35].\n",
            "\n",
            "Il est tentant de s'inspirer des êtres vivants sans les copier naïvement[36] pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux. L'apprentissage automatique reste avant tout un sous-domaine de l'informatique, mais il est étroitement lié opérationnellement aux sciences cognitives, aux neurosciences, à la biologie et à la psychologie, et pourrait à la croisée de ces domaines, nanotechnologies, biotechnologies, informatique et sciences cognitives, aboutir à des systèmes d'intelligence artificielle ayant une assise plus vaste. Des enseignements publics ont notamment été dispensés au Collège de France, l'un par Stanislas Dehaene[37] orienté sur l'aspect bayésien des neurosciences, et l'autre par Yann LeCun[38] sur les aspects théoriques et pratiques de l'apprentissage profond.\n",
            "\n",
            "Enjeux et limites [ modifier | modifier le code ]\n",
            "\n",
            "Quantité et qualité des données [ modifier | modifier le code ]\n",
            "\n",
            "L’apprentissage automatique demande de grandes quantités de données pour fonctionner correctement. Il peut s’avérer difficile de contrôler l’intégrité des jeux de données, notamment dans le cas de données générées par les réseaux sociaux[39].\n",
            "\n",
            "La qualité des « décisions » prises par un algorithme d’AA dépend non seulement de la qualité (donc de leur homogénéité, fiabilité, etc.) des données utilisées pour l’entrainement mais surtout de leur quantité. Donc, pour un jeu de données sociales collecté sans attention particulière à la représentation des minorités, l’AA est statistiquement injuste vis-à-vis de celles-ci. En effet, la capacité à prendre de « bonnes » décisions dépend de la taille des données, or celle-ci sera proportionnellement inférieure pour les minorités.\n",
            "\n",
            "L’AA ne distingue actuellement pas cause et corrélation de par sa construction mathématique, et est incapable d’aller au-delà du cadre imposé par ses données, il n’a donc pas de capacité d’extrapolation.[réf. nécessaire]\n",
            "\n",
            "\n",
            "\n",
            "L’utilisation d’algorithmes d’apprentissage automatique demande donc d’avoir conscience du cadre de données que l’on a utilisé pour l’apprentissage lors de leur utilisation. Il est donc prétentieux d’attribuer des vertus trop grandes aux algorithmes d’apprentissage automatique[40].\n",
            "\n",
            "Biais des algorithmes et des données [ modifier | modifier le code ]\n",
            "\n",
            "Un algorithme peut être biaisé lorsque son résultat dévie par rapport à un résultat neutre, loyal ou équitable. Dans certains cas, les biais algorithmiques peuvent conduire à des situations de discrimination[41].\n",
            "\n",
            "Les données peuvent aussi être biaisées, si l'échantillon de données utilisées pour l'apprentissage du modèle n'est pas neutre et représentatif de la réalité ou déséquilibré. Ce biais est alors appris et reproduit par le modèle[42],[43].\n",
            "\n",
            "Explicabilité et explications des décisions [ modifier | modifier le code ]\n",
            "\n",
            "Les algorithmes d'apprentissage automatique posent des problèmes d'explicabilité globale du système. Si certains modèles comme la régression linéaire ou la régression logistique ont un nombre de paramètres limité et peuvent être interprétés, d'autres types de modèle comme les réseaux de neurones artificiels n'ont pas d'interprétation évidente[44].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Fyw7RFqUSS"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF-0PZVdqNhn"
      },
      "source": [
        "text=\"Cette formation prote sur l'exploitation des outils de machine learning et de deep learning sur les données multimédia (Image, Vidéo et Text) et les données provenenat des systèmes d'information et qui nécessite une analyse de données et un pré-traitement\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evIVhyCXqnzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d6de26-393f-4b25-db57-fce9e4c9837a"
      },
      "source": [
        "obj=TextBlob(text)\n",
        "print(type(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM8Mluz_qt0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eea305c-1bb0-4750-bc4b-6960b486bb5c"
      },
      "source": [
        "print(type(obj))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'textblob.blob.TextBlob'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeHgK9cFx33z",
        "outputId": "44371f38-1cbb-4c5f-b500-df402deb9fb3"
      },
      "source": [
        "print(obj.sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMq4Bqq8qwal"
      },
      "source": [
        "sentiment=obj.sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU-3AHgJq5TT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd03467-86c5-432e-fea5-176aceeeefdd"
      },
      "source": [
        "print(sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iziTdnKq6ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48806949-d6f7-4992-a535-473bf41aba55"
      },
      "source": [
        "ch=\"He is the worst!\"\n",
        "new_obj=TextBlob(ch)\n",
        "sentiment=new_obj.sentiment.polarity\n",
        "print(sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsf7mWpsrlzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4c8381-fe2e-4a02-9e4e-673e732ec881"
      },
      "source": [
        "if sentiment==0:\n",
        "  print(\"The text is neutral\")\n",
        "elif sentiment>0:\n",
        "  print(\"The text is positive\")\n",
        "else:\n",
        "  print(\"The text is negative\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text is negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg9r44BoyVI5"
      },
      "source": [
        "import textblob\n",
        "ch=\"Text\"\n",
        "obj=textblob(ch)\n",
        "sentiment=obj.sentiment.polarity\n",
        "if\n",
        "elif\n",
        "else"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}