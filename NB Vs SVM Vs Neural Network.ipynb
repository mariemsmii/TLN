{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NB Vs SVM Vs Neural Network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"63SLFtNBKmDU"},"source":["# **Packages Importation**"]},{"cell_type":"code","metadata":{"id":"NwisiRC9Krs0","executionInfo":{"status":"ok","timestamp":1635186229134,"user_tz":-60,"elapsed":1406,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}}},"source":["import pandas as pd # pandas is used to read files of the datasets\n","from sklearn.model_selection import train_test_split # train_test_split is used to partionate data into: Train Dataset et Test Dataset\n","from sklearn.naive_bayes import GaussianNB # GaussianNB() is the naive bayes classifier\n","from sklearn.svm import SVC # SVC() is the Support Vector Machines Classifier  \n","from sklearn.neural_network import MLPClassifier # MLPClassifier us the Neural Network Classifier\n","from sklearn.metrics import confusion_matrix, classification_report # Confusion_matrix & classification report are used to evaluate the performance between classifiers"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jR7DWTYTMJgT"},"source":["# **Dataset Preparation**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"dIttLiO-MXEr","executionInfo":{"status":"ok","timestamp":1635186229139,"user_tz":-60,"elapsed":8,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"46dc60c5-e39a-4bf5-bb79-ae6546ee17c5"},"source":["df=pd.read_csv('bill_authentication.csv') # Read the dataset in a new data frame(df)\n","df.head() # Display the first five rows (5 premières lignes)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Variance</th>\n","      <th>Skewness</th>\n","      <th>Curtosis</th>\n","      <th>Entropy</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.62160</td>\n","      <td>8.6661</td>\n","      <td>-2.8073</td>\n","      <td>-0.44699</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.54590</td>\n","      <td>8.1674</td>\n","      <td>-2.4586</td>\n","      <td>-1.46210</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.86600</td>\n","      <td>-2.6383</td>\n","      <td>1.9242</td>\n","      <td>0.10645</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.45660</td>\n","      <td>9.5228</td>\n","      <td>-4.0112</td>\n","      <td>-3.59440</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.32924</td>\n","      <td>-4.4552</td>\n","      <td>4.5718</td>\n","      <td>-0.98880</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Variance  Skewness  Curtosis  Entropy  Class\n","0   3.62160    8.6661   -2.8073 -0.44699      0\n","1   4.54590    8.1674   -2.4586 -1.46210      0\n","2   3.86600   -2.6383    1.9242  0.10645      0\n","3   3.45660    9.5228   -4.0112 -3.59440      0\n","4   0.32924   -4.4552    4.5718 -0.98880      0"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"8JFuJxQ8NBkG","executionInfo":{"status":"ok","timestamp":1635186229139,"user_tz":-60,"elapsed":7,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"553d6b13-4950-4fff-a141-ca2b75088f11"},"source":["df.tail() # Display the last five rows (5 denières lignes)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Variance</th>\n","      <th>Skewness</th>\n","      <th>Curtosis</th>\n","      <th>Entropy</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1367</th>\n","      <td>0.40614</td>\n","      <td>1.34920</td>\n","      <td>-1.4501</td>\n","      <td>-0.55949</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1368</th>\n","      <td>-1.38870</td>\n","      <td>-4.87730</td>\n","      <td>6.4774</td>\n","      <td>0.34179</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1369</th>\n","      <td>-3.75030</td>\n","      <td>-13.45860</td>\n","      <td>17.5932</td>\n","      <td>-2.77710</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1370</th>\n","      <td>-3.56370</td>\n","      <td>-8.38270</td>\n","      <td>12.3930</td>\n","      <td>-1.28230</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1371</th>\n","      <td>-2.54190</td>\n","      <td>-0.65804</td>\n","      <td>2.6842</td>\n","      <td>1.19520</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Variance  Skewness  Curtosis  Entropy  Class\n","1367   0.40614   1.34920   -1.4501 -0.55949      1\n","1368  -1.38870  -4.87730    6.4774  0.34179      1\n","1369  -3.75030 -13.45860   17.5932 -2.77710      1\n","1370  -3.56370  -8.38270   12.3930 -1.28230      1\n","1371  -2.54190  -0.65804    2.6842  1.19520      1"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"U1MoxdGUNQxm"},"source":["We  notice that:\n","*   We have **4 features**: Variance, Skewness, Curtosis and Entropy;\n","*   We have **2 classes**: Class 0 and Class 1;\n","*   We have at all **1372 samples**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aVrefgAKN1Pa"},"source":["# **Partitioning Data**"]},{"cell_type":"markdown","metadata":{"id":"i24kbs6MOzGr"},"source":["[X_train,X_test,y_train,y_test]=train_test_split(*X,y*,**test_size=0.2**)\n","This function create two parititions of the dataset with a test size of 0.2:\n","* Train dataset (**80%** of the overall dataset)\n","* Test dataset  (**20%** of the overall dataset) \n"]},{"cell_type":"markdown","metadata":{"id":"SG4wrlikPvc7"},"source":["* X denotes the matrix of features X-> delete from df the coloumn class\n","* y denotes the label coloumn y-> troncate the df only on the coloumn class"]},{"cell_type":"code","metadata":{"id":"lL503vlmQAgo","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1635186230897,"user_tz":-60,"elapsed":5,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"a18faf77-7094-4ff0-9775-9c4c726fb7f8"},"source":["X=df.drop('Class',axis=1)\n","y=df['Class']\n","X.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Variance</th>\n","      <th>Skewness</th>\n","      <th>Curtosis</th>\n","      <th>Entropy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.62160</td>\n","      <td>8.6661</td>\n","      <td>-2.8073</td>\n","      <td>-0.44699</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.54590</td>\n","      <td>8.1674</td>\n","      <td>-2.4586</td>\n","      <td>-1.46210</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.86600</td>\n","      <td>-2.6383</td>\n","      <td>1.9242</td>\n","      <td>0.10645</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.45660</td>\n","      <td>9.5228</td>\n","      <td>-4.0112</td>\n","      <td>-3.59440</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.32924</td>\n","      <td>-4.4552</td>\n","      <td>4.5718</td>\n","      <td>-0.98880</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Variance  Skewness  Curtosis  Entropy\n","0   3.62160    8.6661   -2.8073 -0.44699\n","1   4.54590    8.1674   -2.4586 -1.46210\n","2   3.86600   -2.6383    1.9242  0.10645\n","3   3.45660    9.5228   -4.0112 -3.59440\n","4   0.32924   -4.4552    4.5718 -0.98880"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"33Yea7kVN4li","executionInfo":{"status":"ok","timestamp":1635186231315,"user_tz":-60,"elapsed":1,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}}},"source":["[X_train,X_test,y_train,y_test]=train_test_split(X,y,test_size=0.2)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGs7BRjlQMN8"},"source":["* Train dataset = 80% * Number of samples (1372) = 1372 * 0.8\n","* Test  dataset = 20% * Number of samples (1372) = 1372 * 0.2\n","* A.N: Train dataset = 1097.6 & Test  dataset = 274.4"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llzg77LeQKpE","executionInfo":{"status":"ok","timestamp":1635186232108,"user_tz":-60,"elapsed":5,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"d96dc65e-9051-4d2d-910b-be31be75396c"},"source":["print(\"Train dataset size: {}/{}\".format(len(X_train),len(y)))\n","print(\"Test dataset size: {}/{}\".format(len(X_test),len(y)))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset size: 1097/1372\n","Test dataset size: 275/1372\n"]}]},{"cell_type":"markdown","metadata":{"id":"db96tgaMTOky"},"source":["\n","\n","*   X_train: Features of train;\n","*   y_train: Labels of X_train;\n","*   X_test : Fetaures of test;\n","*   y_test : Labels of X_test. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"d4Vk6NxWTncA"},"source":["# **Machine Learning: NB Vs SVM Vs Neural Network**"]},{"cell_type":"markdown","metadata":{"id":"ChXcbWEMTxux"},"source":["We will compare between these 3 classifiers on the same partitioned data. Let's start by the initialization of the classifier which we will compare."]},{"cell_type":"code","metadata":{"id":"-WiYklfJT7YO","executionInfo":{"status":"ok","timestamp":1635186234274,"user_tz":-60,"elapsed":3,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}}},"source":["gnb=GaussianNB() # gnb is a naive bayes classifier\n","linear_svm  =SVC(kernel='linear') # linear_svm is a Linear Support Vectors\n","rbf_svm     =SVC(kernel='rbf')    # rbf_svm is a RBF support vectors\n","sigmoid_svm =SVC(kernel='sigmoid')# sigmoid support vectors\n","ploy_svm    =SVC(kernel='poly',degree=2) # Ploynom with degree=2 as support vectors \n","neural=MLPClassifier(hidden_layer_sizes=(100,20),activation='relu',solver='adam') # neural is a neural network classification "],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZR4kFO_QVtii"},"source":["neural=MLPClassifier parametres:\n","*   hidden_layer_sizes=(100,20):   4x100x20x2\n","*   activation='relu': activation function in all neurons is Relu(x)\n","*   solver='adam'    : algorithm for weights' update during the training\n","*   defalut value of learning rate (alph): 0.001\n"]},{"cell_type":"markdown","metadata":{"id":"Ym3DR-IpW9Py"},"source":["Now, we will move to the training process with using of the fit() function.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KIvuHqsT7Xx","executionInfo":{"status":"ok","timestamp":1635186247304,"user_tz":-60,"elapsed":1024,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"f51cf72c-cc45-4e1a-ca17-dba2b5ab5921"},"source":["gnb.fit(X_train,y_train) # Train Guassian NB classifier \n","linear_svm.fit(X_train,y_train) # Train SVM\n","rbf_svm.fit(X_train,y_train)\n","sigmoid_svm.fit(X_train,y_train)\n","ploy_svm.fit(X_train,y_train)\n","neural.fit(X_train,y_train) # Train Neural Network - finding the best weight matrix"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","              hidden_layer_sizes=(100, 20), learning_rate='constant',\n","              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n","              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n","              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n","              tol=0.0001, validation_fraction=0.1, verbose=False,\n","              warm_start=False)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"qOjbh5LTXtnR"},"source":["Now, we will test the learned models!\n","\n","\n","* We will ask the model to give a prediction based on its learning \n","* Each Classifier will produce a prediction; y_nb,y_linear_svm,etc. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"eO9kqm5GZELx"},"source":["We have two types of labels: \n","*   y_test: true label coming from the initial dataset\n","*   y_nb, y_linear_svm, y_rbf_svm, y_sigmoid_svm, y_ploy_svm et y_neural: are the labels predicted by the models: naive bayes, svm with all kernels and neural network\n","!!! Le modèle est performant si et seulement si sa prédiction ègale aux vrais labels !!!"]},{"cell_type":"code","metadata":{"id":"RnAwzGQ1XqxY","executionInfo":{"status":"ok","timestamp":1635186304033,"user_tz":-60,"elapsed":563,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}}},"source":["y_nb=gnb.predict(X_test)\n","y_linear_svm=linear_svm.predict(X_test)\n","y_rbf_svm=rbf_svm.predict(X_test)\n","y_ploy_svm=ploy_svm.predict(X_test)\n","y_sigmoid_svm=sigmoid_svm.predict(X_test)\n","y_neural=neural.predict(X_test)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UaBVIhqvZ0Kn"},"source":["#  **Performance Evaluation**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nRr0Me2Z5QP","executionInfo":{"status":"ok","timestamp":1635186306030,"user_tz":-60,"elapsed":527,"user":{"displayName":"wael ouarda","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjU-0xbwqOs-jyGg95NtY_TRDiG1JPRC2lv6VmkQg=s64","userId":"10121207919391015595"}},"outputId":"9a52c93d-5977-479a-d849-c9177148898b"},"source":["print ('************* Peformance Evauation of Naive Bayes **************')\n","print(confusion_matrix(y_test,y_nb))\n","print(classification_report(y_test,y_nb))\n","print ('************* Peformance Evauation of Linear SVM **************')\n","print(confusion_matrix(y_test,y_linear_svm))\n","print(classification_report(y_test,y_linear_svm))\n","print ('************* Peformance Evauation of RBF SVM **************')\n","print(confusion_matrix(y_test,y_rbf_svm))\n","print(classification_report(y_test,y_rbf_svm))\n","print ('************* Peformance Evauation of Sigmoid SVM **************')\n","print(confusion_matrix(y_test,y_sigmoid_svm))\n","print(classification_report(y_test,y_sigmoid_svm))\n","print ('************* Peformance Evauation of Polynomial (2) SVM **************')\n","print(confusion_matrix(y_test,y_ploy_svm))\n","print(classification_report(y_test,y_ploy_svm))\n","print ('************* Peformance Evauation of Neural Network **************')\n","print(confusion_matrix(y_test,y_neural))\n","print(classification_report(y_test,y_neural))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["************* Peformance Evauation of Naive Bayes **************\n","[[129  17]\n"," [ 29 100]]\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.88      0.85       146\n","           1       0.85      0.78      0.81       129\n","\n","    accuracy                           0.83       275\n","   macro avg       0.84      0.83      0.83       275\n","weighted avg       0.83      0.83      0.83       275\n","\n","************* Peformance Evauation of Linear SVM **************\n","[[143   3]\n"," [  0 129]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99       146\n","           1       0.98      1.00      0.99       129\n","\n","    accuracy                           0.99       275\n","   macro avg       0.99      0.99      0.99       275\n","weighted avg       0.99      0.99      0.99       275\n","\n","************* Peformance Evauation of RBF SVM **************\n","[[144   2]\n"," [  0 129]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.99      0.99       146\n","           1       0.98      1.00      0.99       129\n","\n","    accuracy                           0.99       275\n","   macro avg       0.99      0.99      0.99       275\n","weighted avg       0.99      0.99      0.99       275\n","\n","************* Peformance Evauation of Sigmoid SVM **************\n","[[106  40]\n"," [ 50  79]]\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.73      0.70       146\n","           1       0.66      0.61      0.64       129\n","\n","    accuracy                           0.67       275\n","   macro avg       0.67      0.67      0.67       275\n","weighted avg       0.67      0.67      0.67       275\n","\n","************* Peformance Evauation of Polynomial (2) SVM **************\n","[[140   6]\n"," [  0 129]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.96      0.98       146\n","           1       0.96      1.00      0.98       129\n","\n","    accuracy                           0.98       275\n","   macro avg       0.98      0.98      0.98       275\n","weighted avg       0.98      0.98      0.98       275\n","\n","************* Peformance Evauation of Neural Network **************\n","[[146   0]\n"," [  0 129]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00       146\n","           1       1.00      1.00      1.00       129\n","\n","    accuracy                           1.00       275\n","   macro avg       1.00      1.00      1.00       275\n","weighted avg       1.00      1.00      1.00       275\n","\n"]}]}]}